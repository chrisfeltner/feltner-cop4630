{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework4ChristopherFeltner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisfeltner/feltner-cop4630/blob/master/HW4/Homework4ChristopherFeltner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJlmaUSqsuAO",
        "colab_type": "text"
      },
      "source": [
        "#Homework 4\n",
        "##Christopher Feltner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMIERZfMs1Ug",
        "colab_type": "text"
      },
      "source": [
        "#Part 1: General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w-6ij8cyk_K",
        "colab_type": "text"
      },
      "source": [
        "##Artificial Intelligence\n",
        "###\"The science and technology of making intelligent machines\"\n",
        "###A branch of computer science dealing with the simulation of intelligent behavior in computers\n",
        "\n",
        "Symbolic AI or GOFAI is a collection of AI methods which use high-level, manual representation of problems and logic. It is therefore heavily reliant on human intervention.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EW-lcFzmUn",
        "colab_type": "text"
      },
      "source": [
        "##Machine Learning\n",
        "###A subset of AI\n",
        "Machine learning programs adjust themselves in response to data they have been exposed to. It is dynamic and does not require human intervention to learn, making it less reliant on human experts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo-OLtLE0HGY",
        "colab_type": "text"
      },
      "source": [
        "##Deep Learning\n",
        "###A subset of machine learning\n",
        "Machine learning models with numerous hidden layers are used, with different layers learning different features of the data. This produces more accurate predictions from the data and allows the model to solve more difficult problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pydCx3m52YfG",
        "colab_type": "text"
      },
      "source": [
        "##Types of Machine Learning\n",
        "###Supervised Learning\n",
        "The model is provided with data which has been labelled (presumably by humans). The labelled data is then used during training of the model.\n",
        "###Unsupervised Learning\n",
        "The model is provided with an unlabelled dataset. The goal of the model is to identify meaningful patterns in the data. Because the data is unlabelled, the model must infer its own rules for categorizing the data.\n",
        "###Reinforcement Learning\n",
        "In reinforcement learning, the model is provided a means to interact with an environment (i.e. a game, a virtual environment) to produce data. A negative or positive incentive is applied to undesireable or desireable behavior (i.e. a negative incentive is applied to the model getting a game over screen in a videogame). The agent then learns to take action to maximize its reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7x_KWw4nsB",
        "colab_type": "text"
      },
      "source": [
        "##ML Terminology\n",
        "*   label -- what we wish for the model to predict\n",
        "*   feature -- an input variable\n",
        "*   example -- a particular instance of data\n",
        "*   training -- showing the model labelled examples in hopes that the model will learn the relationships between features and labels\n",
        "*   inference -- applying the trained model to unlabeled examples\n",
        "*   regression model -- a model which predicts continuous values\n",
        "*   classification model -- predicts discrete values\n",
        "*   loss -- a numerical measure of how bad a model's prediction was on an example\n",
        "*   Mean Squared Error (MSE) -- average squared loss for the entire dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcEhvUcX6Zg5",
        "colab_type": "text"
      },
      "source": [
        "#Part 2: Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GZvra3-6fdd",
        "colab_type": "text"
      },
      "source": [
        "###Neuron Structure\n",
        "Neurons take in the values of previous neurons connected to it multiplied by their weights as input, sums them, adds a bias, and then passes this result through an activation function as output.\n",
        "###Activation Functions\n",
        "The purpose of an activation function is to scale the output of a neuron to a value between 0 and 1\n",
        "####Sigmoid\n",
        "Sigmoid is often used for binary classification\n",
        "\\begin{equation}\n",
        "\\sigma = \\frac{1}{1+e^{-x}}\n",
        "\\end{equation}\n",
        "\n",
        "####Tanh\n",
        "\\begin{equation}\n",
        "\\tanh{x}\n",
        "\\end{equation}\n",
        "\n",
        "####ReLU\n",
        "ReLU is often used in hidden layers\n",
        "\\begin{equation}\n",
        "\\max(0, x)\n",
        "\\end{equation}\n",
        "\n",
        "####Leaky ReLU\n",
        "\\begin{equation}\n",
        "\\max(0.1x,x)\n",
        "\\end{equation}\n",
        "\n",
        "####Softmax\n",
        "An activation function used for classifying into non-binary categories, which outputs a probability distribution\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QJamXNEAA5x",
        "colab_type": "text"
      },
      "source": [
        "###Sequential Neural Network with Dense Layers\n",
        "Each neuron in a layer are connected to each neuron in the next layer. Each neuron after the first layer recieves as input all outputs from every neuron in the previous layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4_aVupEAdza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.model import Sequential\n",
        "model = Sequential();\n",
        "model.add(Dense(5, input_dim=5, activation='relu'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvdMogooG7yk",
        "colab_type": "text"
      },
      "source": [
        "###Convolutional Neural Network\n",
        "A convolutional neural network is used to classify images. A convnet is structured in two parts: a network of convolutional and pooling layers which learn the features of images, and a fully connected network to classify images based on the output of the convolutional layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSF0tihAHVMj",
        "colab_type": "text"
      },
      "source": [
        "###Feature Network\n",
        "####Convolutional Layers\n",
        "In a convolutional layer, feature maps slide over the image horizontally and vertically one pixel at a time and the convolution operation is used, which produces a new output feature map. During training, the CNN learns the optimal values for the feature maps to extract meaningful features from the images.\n",
        "####Pooling Layers\n",
        "Pooling layers are used to downsample feature maps while retaining feature information. Typically, maxpooling is used, where the maximum value of the feature map is output to a new feature map and the remaining values are discarded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9EO8OkQKWqg",
        "colab_type": "text"
      },
      "source": [
        "###Fully Connected Network\n",
        "The fully connected network takes in the flattened feature vector from the convolutional network as input and classifies the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3xDTuxtKmI1",
        "colab_type": "text"
      },
      "source": [
        "###Keras Convnet Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKvlrU77KxJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "#Convolutional Part\n",
        "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(1, 28, 28)))\n",
        "model.add(Conv2D(64, activation='relu'))\n",
        "model.add(MaxPooling(pool_size=(2, 2)))\n",
        "#Fully Connected Part\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mOLOGrGL2R6",
        "colab_type": "text"
      },
      "source": [
        "#Part 3: Compiling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi2Ncft6L48r",
        "colab_type": "text"
      },
      "source": [
        "##Optimization\n",
        "Optimizers updates the model in response to the results of the loss function. A machine learning model is initialized with random values for the weights and biases. The optimizer is the function used to update the weights and biases during training in order to produce better results, in an effort to find the weights and biases which produce the lowest possible loss\n",
        "###Gradient Descent\n",
        "Gradient Descent is an optimizer. With each time step, the model's weights are adjusted in the direction of their negative gradient. The adjustment size is the learning rate.\n",
        "###Stochastic Gradient Descent\n",
        "Wheras in Gradient Descent, where all training examples are used for each training step, in the SGD optimizer only a subset of the training examples are used, which is defined by the batch size, in order to produce training results faster.\n",
        "##Compilation Parameters Used in Training\n",
        "###Hyperparameters\n",
        "Hyperparameters are parameters which are set before training that can be used to tune the model during training. They are usually set manually by the engineer. Learning rate is an example of a hyperparameter.\n",
        "###Learning Rate\n",
        "The learning rate is the amount that the model's weights are changed at each time step. If the learning rate is too large, the model could diverge and never end up finding a local minimum of the loss function. If the learning rate is too small, the model will take too long to train and will not be able to find a local minimum of the loss because it is training too slowly.\n",
        "###Loss Function\n",
        "The function which defines how well the model is performing. It is used during training to adjust the weights. Commonly used losses include Mean Squared Error and Categorical Crossentropy.\n",
        "###Regularization\n",
        "The goal of regularization is to prevent overfitting. Part of the loss function penalizes large weight values, even if the prediction is correct. This helps the model to generalize better.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98NExXjqQZF5",
        "colab_type": "text"
      },
      "source": [
        "##Compiling a Model\n",
        "This model will train using Mean Squared Error as the loss function, a learning rate of 0.01, and Stochastic Gradient Descent as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-6JSopKQYd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=losses.mean_squared_error, lr=0.01, optimizer='sgd')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE7fYn5YQ-t1",
        "colab_type": "text"
      },
      "source": [
        "#Part 4: Training\n",
        "Training is the process where the model is exposed to the training data in an effort to find optimal values for the weights/biases/filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LKKntHTROvq",
        "colab_type": "text"
      },
      "source": [
        "##Overfitting\n",
        "When a model learns features which are too specific to the training data and is unable to generalize well to testing data. Training loss/accuracy continues to increase but validation loss/accuracy fails to improve. This happens when a model is too complex for the training data. Having too many training epochs can also cause the model to overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT-BPbrSTPkj",
        "colab_type": "text"
      },
      "source": [
        "###Overfitting Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaoR04yDTRjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93f5d395-a351-4051-ff15-3899ac4e82e3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "history = model.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs,  \n",
        "                      validation_data=(test_images, test_labels))\n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "test_loss_values = history_dict['val_loss']\n",
        "epochs_range = range(1, epochs+1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epochs_range, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')\n",
        "plt.title('Training and test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 1.1575 - acc: 0.7882 - val_loss: 0.5025 - val_acc: 0.8242\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.4602 - acc: 0.8376 - val_loss: 0.6033 - val_acc: 0.7886\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.4208 - acc: 0.8509 - val_loss: 0.4127 - val_acc: 0.8516\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3905 - acc: 0.8605 - val_loss: 0.4240 - val_acc: 0.8540\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3716 - acc: 0.8688 - val_loss: 0.4358 - val_acc: 0.8505\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3546 - acc: 0.8745 - val_loss: 0.4059 - val_acc: 0.8548\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3489 - acc: 0.8763 - val_loss: 0.4376 - val_acc: 0.8492\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.3385 - acc: 0.8792 - val_loss: 0.3887 - val_acc: 0.8698\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 5s 76us/sample - loss: 0.3222 - acc: 0.8842 - val_loss: 0.4017 - val_acc: 0.8662\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.3127 - acc: 0.8871 - val_loss: 0.4226 - val_acc: 0.8526\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3097 - acc: 0.8888 - val_loss: 0.3761 - val_acc: 0.8759\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3103 - acc: 0.8903 - val_loss: 0.3783 - val_acc: 0.8693\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.2942 - acc: 0.8931 - val_loss: 0.3972 - val_acc: 0.8797\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.2864 - acc: 0.8968 - val_loss: 0.4199 - val_acc: 0.8570\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.2871 - acc: 0.8966 - val_loss: 0.3995 - val_acc: 0.8746\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.2840 - acc: 0.8985 - val_loss: 0.3706 - val_acc: 0.8730\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.2743 - acc: 0.9013 - val_loss: 0.4089 - val_acc: 0.8718\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.2832 - acc: 0.8996 - val_loss: 0.4101 - val_acc: 0.8757\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2797 - acc: 0.9004 - val_loss: 0.4930 - val_acc: 0.8767\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2680 - acc: 0.9024 - val_loss: 0.4050 - val_acc: 0.8755\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2553 - acc: 0.9075 - val_loss: 0.4479 - val_acc: 0.8797\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2948 - acc: 0.9031 - val_loss: 0.4424 - val_acc: 0.8682\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2669 - acc: 0.9050 - val_loss: 0.4217 - val_acc: 0.8771\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2528 - acc: 0.9078 - val_loss: 0.4477 - val_acc: 0.8814\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.2624 - acc: 0.9065 - val_loss: 0.3897 - val_acc: 0.8756\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2473 - acc: 0.9115 - val_loss: 0.4984 - val_acc: 0.8829\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.2487 - acc: 0.9102 - val_loss: 0.7341 - val_acc: 0.8744\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2586 - acc: 0.9080 - val_loss: 0.7879 - val_acc: 0.8677\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.2431 - acc: 0.9125 - val_loss: 0.6355 - val_acc: 0.8772\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2489 - acc: 0.9119 - val_loss: 0.6421 - val_acc: 0.8771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL36CCkFn5aDXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWINkYbYUGWN",
        "colab_type": "text"
      },
      "source": [
        "##Underfitting\n",
        "A model is too simple and is unable to describe the data, or was not trained in such a way that it learns the optimal weights. This can happen when there are too few trainable parameters to describe the number of features in the data or when there are too few training epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA5EOrt0VpSz",
        "colab_type": "text"
      },
      "source": [
        "##Ways of Fighting Overfitting\n",
        "###Dropout\n",
        "Dropout ignores a certain percentage of neurons, chosen at random, in a forwards or backwards pass, meaning that other neurons will need to \"pick up the slack\" and learn some of the features known by the dropped out neurons. This prevents some neurons from gaining too much power in the prediction, and develops co-dependency among the neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM-_IRTXWZV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu'))\n",
        "#Dropout 50% of neurons\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W43WN7DWzFr",
        "colab_type": "text"
      },
      "source": [
        "###Data Augmentation\n",
        "Another way of fighting overfitting is to add more training and validation data. A way of simulating new data without actually collecting new data is to use data augmentation: slightly altering data without changing the features. For example, in image data, a researcher could crop, horizontally flip, or pad the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9nq86r0XNyM",
        "colab_type": "text"
      },
      "source": [
        "#Part 5: Finetuning a Trained Model\n",
        "A pretrained model can be used as a base for a classifier of a set of classes different than the original model was trained for. This is because complex models, such as NASNet or InceptionV3, extract features which are common to most images and generalize well in their top layers. To make this new model, we add a new classifier to the end of the pretraiend model. Fine tuning requires two stages. In the first, we train a the combined model, but freeze the layers of the base model in order to train the classifier. Next, we unfreeze some of the layers near the end of the base model and train them in conjunction with the classifier in order to extract features specific to the new classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH4aPTi1a3jx",
        "colab_type": "text"
      },
      "source": [
        "##Freezing\n",
        "Freezing a layer means to prevent it from training. Here is how we can freeze a layer in Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwWKFcL2bAFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers[:249]:\n",
        "   layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ojw_si7bGbC",
        "colab_type": "text"
      },
      "source": [
        "##Fine Tuning: Part One\n",
        "We add a classifier (implemented as a fully connected layer) to the end of our base model. Then, we freeze the layers in our base model, only training our classifier on the results of the original base model using the new data with ten classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyTtBewdbS6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "#Freeze base model\n",
        "base_model.trainable=False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRx1429TcLot",
        "colab_type": "text"
      },
      "source": [
        "##Fine Tuning: Part Two\n",
        "Now that our classifier has been trained on the output from the base model, we unfreeze some of the base model's layers and train it jointly with the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQczC55ccrWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}